{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3504815d",
   "metadata": {},
   "source": [
    "# Калибровки модели на open source моделе Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb04cdd",
   "metadata": {},
   "source": [
    "Выполняется на kaggle с использованием GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65293016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall -y bitsandbytes peft transformers\n",
    "\n",
    "# Установка совместимых версий для CUDA 12.1\n",
    "!pip install -q torch==2.3.1+cu121 \\\n",
    "               torchvision==0.18.1+cu121 \\\n",
    "               torchaudio==2.3.1 \\\n",
    "               --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install -q bitsandbytes==0.45.0 \\\n",
    "               transformers==4.41.1 \\\n",
    "               peft==0.11.0 \\\n",
    "               accelerate==0.29.3\n",
    "\n",
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06fe6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка окружения для CUDA 12.1\n",
    "import os\n",
    "os.environ[\"BNB_CUDA_VERSION\"] = \"121\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.1/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbaf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"token\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"IlyaGusev/saiga_mistral_7b_lora\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=dict(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"IlyaGusev/saiga_mistral_7b_lora\",\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392edc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Состояние агента\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_query: str\n",
    "    profile: str\n",
    "    search_performed: bool\n",
    "    search_results: str\n",
    "\n",
    "class Agent_opensourceLLM:\n",
    "    \"\"\"LangGraph агент\"\"\"\n",
    "    biases = ['стадность','излишняя самоуверенность']\n",
    "    sex = {1: 'мужчина', 2: 'женщина'}\n",
    "    tip = {1:'Москва или Санкт-Петербург',\n",
    "       2: 'город - миллионник (численность людей)',\n",
    "       3: 'городе ,где проживают 500-950 тыс. человек',\n",
    "       4: 'городе ,где проживают 100-500 тыс. человек',\n",
    "       5: 'городе ,где проживают до 100 тыс. человек',\n",
    "       6: 'селе',\n",
    "       7: 'неизвестно, что за город'}\n",
    "    fo =  {1: 'Центральный федеральный округ',\n",
    "       2: 'Северо-Западный федеральный округ',\n",
    "       3: 'Южный федеральный округ',\n",
    "       4: 'Северо-Кавказский федеральный округ',\n",
    "       5: 'Приволжский федеральный округ',\n",
    "       6: 'Уральский федеральный округ',\n",
    "       7: 'Сибирский федеральный округ',\n",
    "       8: 'Дальневосточный федеральный округ'}\n",
    "    prof = {1: 'неработающий пенсионер',\n",
    "       2: 'работающий пенсионер',\n",
    "       3: 'неработающий учащийся, студент',\n",
    "       4: 'работающий учащийся, студент',\n",
    "       5: 'безработный',\n",
    "       6: 'находящийся в декретном отпуске',\n",
    "       7: 'работающий в найме',\n",
    "       8: 'предприниматель',\n",
    "       9: 'самозанятый',\n",
    "       99: '-',\n",
    "       999: '-'}\n",
    "    dohod = {1:'очень хорошее',\n",
    "       2: 'хорошее',\n",
    "       3: 'среднее',\n",
    "       4: 'плохое',\n",
    "       5: 'очень плохое',\n",
    "       99: 'неизвестно'}\n",
    "    edu = {1:'неполное среднее образование',\n",
    "       2: 'среднее образование (школа или ПТУ)',\n",
    "       3: 'среднее специальное образование (техникум)',\n",
    "       4: 'незаконченное высшее (с 3-го курса ВУЗа)',\n",
    "       5: 'высшее образование',\n",
    "       6: 'неизвестное образование',\n",
    "       999: 'неизвестное образование'}\n",
    "    def __init__(self, model, tokenizer, row):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.row = row        \n",
    "        self.bias = np.random.choice(self.biases,1)[0]\n",
    "        self.graph = self._create_graph()\n",
    "\n",
    "    def _generate_text(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        # Форматируем промпт в формате Saiga/Mistral\n",
    "        prompt = f\"<s>system\\n{system_prompt}</s>\\n<s>user\\n{user_prompt}</s>\\n<s>bot\\n\"\n",
    "    \n",
    "        # Токенизация и генерация\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "        # Декодинг и очистка ответа\n",
    "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True) \n",
    "        response = self._extract_generated_text(full_response)\n",
    "        return response\n",
    "\n",
    "    def _extract_generated_text(self, text: str) -> str:\n",
    "        \"\"\"Извлекает только сгенерированный текст (последний ответ бота)\"\"\"\n",
    "        # Оставить только текст после последнего <s>bot\\n\n",
    "        if \"<s>bot\\n\" in text:\n",
    "            text = text.split(\"<s>bot\\n\")[-1]\n",
    "        # Обрезать по первому </s>\n",
    "        if \"</s>\" in text:\n",
    "            text = text.split(\"</s>\")[0]\n",
    "        # Удалить все служебные теги\n",
    "        text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "        text = text.replace(\"system\\n\", \"\").replace(\"user\\n\", \"\").replace(\"bot\\n\", \"\")\n",
    "        # Оставить только последний абзац (если всё равно что-то не то)\n",
    "        paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "            \n",
    "        if len(paragraphs) > 1:\n",
    "            return '\\n'.join(paragraphs[-2:])  # последние 2 \n",
    "        return text.strip()\n",
    "\n",
    "    def create_profile(self): \n",
    "        '''Создает промпт на основе demographics_info и bias'''\n",
    "        profile_prompt = f\"\"\"Представь, что сейчас 2023 год, ТЫ {self.prof[self.row['PROF']]} {self.sex[self.row['SEX']]} {self.row['AGE']} лет, проживающий в России ({self.fo[self.row['FO']]}) в {self.tip[self.row['TIP']]}. Твое материальное состояние можно охарактеризовать, как {self.dohod[self.row['DOHOD']]}. Ты получил {self.edu[self.row['EDU']]}. Тебе присуща {self.bias}.    \n",
    "        \"\"\"\n",
    "        return profile_prompt\n",
    "\n",
    "    def _create_graph(self):\n",
    "        \"\"\"Создает LangGraph граф\"\"\"\n",
    "        \n",
    "        # Определяем граф\n",
    "        graph_builder = StateGraph(AgentState)\n",
    "        \n",
    "        # Добавляем nodes\n",
    "        graph_builder.add_node(\"initialize_profile\", self._initialize_profile_node)\n",
    "        graph_builder.add_node(\"search\", self._search_node)\n",
    "        graph_builder.add_node(\"generate_response\", self._generate_response_node)\n",
    "        \n",
    "        # Добавляем edges\n",
    "        graph_builder.add_edge(START, \"initialize_profile\")\n",
    "        graph_builder.add_edge(\"initialize_profile\", \"search\")  \n",
    "        graph_builder.add_edge(\"search\", \"generate_response\")\n",
    "        graph_builder.add_edge(\"generate_response\", END)\n",
    "        \n",
    "        return graph_builder.compile()\n",
    "\n",
    "    def _initialize_profile_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Node для инициализации профиля\"\"\"\n",
    "        profile = self.create_profile()\n",
    "        return {\n",
    "            **state,\n",
    "            \"profile\": profile,\n",
    "            \"search_performed\": False,\n",
    "            \"search_results\": \"\"\n",
    "        }\n",
    "        \n",
    "    def _search_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Node для поиска с обработкой ошибок\"\"\"\n",
    "        search_query = self._create_search_query(state)\n",
    "            \n",
    "        return {\n",
    "            **state,\n",
    "            \"search_performed\": True,\n",
    "            \"search_results\": search_query\n",
    "            }\n",
    "\n",
    "    def _create_search_query(self, state: AgentState) -> str:\n",
    "        \"\"\"Создает поисковый запрос\"\"\"\n",
    "        query = f\"\"\"{state['profile']}\\nОпиши текущую экономическую ситуацию для меня с учетом МОИХ демо-географических характеристик на момент 2023 года.\"\"\"\n",
    "        system_prompt = \"\"\"Ты ИИ-ассистент, который помогает создать подробный портрет человека\"\"\"\n",
    "        return self._generate_text(system_prompt, query)\n",
    "\n",
    "\n",
    "    def _generate_response_node(self, state: AgentState) -> AgentState:\n",
    "        system_prompt = f\"\"\"{state['profile']}\\n {state['search_results']}\\n \n",
    "        Ответь на вопрос от первого лица: {state['user_query']}.\n",
    "        \"\"\"\n",
    "        response = self._generate_text(system_prompt, state['user_query'])\n",
    "        response = self._extract_generated_text(response)\n",
    "        return {\n",
    "                **state,\n",
    "            \"messages\": [AIMessage(content=response)]\n",
    "        }\n",
    "\n",
    "    def process_query(self, query: str) -> str:\n",
    "        \"\"\"Обрабатывает запрос пользователя через LangGraph\"\"\"\n",
    "        try:\n",
    "            # Инициализируем состояние\n",
    "            initial_state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"profile\": \"\",\n",
    "                \"search_performed\": False,\n",
    "                \"search_results\": \"\"\n",
    "            }\n",
    "            \n",
    "            # Запускаем граф\n",
    "            result = self.graph.invoke(initial_state)\n",
    "            \n",
    "            # Возвращаем последнее сообщение\n",
    "            if result.get(\"messages\") and len(result[\"messages\"]) > 0:\n",
    "                return result[\"messages\"][-1].content\n",
    "            else:\n",
    "                return \"Извините, не удалось получить ответ.\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Ошибка при обработке запроса: {str(e)}\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
